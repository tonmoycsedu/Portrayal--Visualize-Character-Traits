{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from spacy.symbols import nsubj, VERB, amod, acomp\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "## load spacy and neural coref\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "trf_nlp = spacy.load(\"en_core_web_trf\")\n",
    "nlp.max_length = 2000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load AllenNLP models\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "import allennlp_models.tagging\n",
    "from allennlp_models import pretrained\n",
    "\n",
    "coref_predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2021.03.10.tar.gz\")\n",
    "sentiment_predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/stanford-sentiment-treebank-roberta.2021-03-11.tar.gz\")\n",
    "open_info_predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/openie-model.2020.03.26.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mother’s Day\\nWorking on any Sunday was bad enough.'"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load story\n",
    "story = \"Mother's_Day\"\n",
    "file_name = \"../static/data/story/\"+ story + \"/\"+ story +\".txt\"\n",
    "book = open(file_name, encoding='utf-8-sig').read()\n",
    "book[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "book = book.replace('“','\"').replace('”','\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "book1 = \" \".join(book.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split using regex and coreference resolution\n",
    "##### We only need to run this one time and save the clusters in a json file. The json file is then fed into the interface to assign names to the cluster and merge them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "## regex to split text into parts and chapters\n",
    "regex1 = \"PART\\s+(?:ONE|TWO|THREE|FOUR|FIVE|SIX|SEVEN|EIGHT.*)\"\n",
    "regex2 = \"Chapter\\s+\\d+\"\n",
    "# regex2 = \"Chapter\\s+?(IX|IV|V?I{0,3}).*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_range(book):\n",
    "    all_clusters = []\n",
    "    for m in re.finditer(regex1, book):\n",
    "        part_start = m.start()\n",
    "        ## start of a part\n",
    "        all_clusters.append({\"start_char\": part_start})\n",
    "\n",
    "    for i, part in enumerate(all_clusters):\n",
    "        chapters = []\n",
    "        if i == (len(all_clusters) -1):\n",
    "            part['end_char'] = len(book)\n",
    "            for m in re.finditer(regex2, book[part['start_char']:]):\n",
    "                ## start of a chapter\n",
    "                chapter_start = m.end()\n",
    "                chapters.append({'start_char':chapter_start + part['start_char'], 'span':m.span()})\n",
    "#         else:\n",
    "#             part['end_char'] = all_clusters[i + 1]['start_char']\n",
    "#             for m in re.finditer(regex2, book[part['start_char']: all_clusters[i + 1]['start_char']]):\n",
    "#                 chapter_start = m.start()\n",
    "#                 chapters.append({'start_char':chapter_start + part['start_char']})\n",
    "\n",
    "        all_clusters[i]['chapters'] = chapters\n",
    "        \n",
    "    ## get the end of the chapters\n",
    "    for i, part in enumerate(all_clusters):\n",
    "        for j, chapter in enumerate(part['chapters']):\n",
    "            if j == (len(part['chapters']) -1):\n",
    "                chapter['end_char'] = part['end_char']   \n",
    "            else:\n",
    "                chapter['end_char'] = part['chapters'][j+1]['span'][0]\n",
    "\n",
    "    return all_clusters\n",
    "\n",
    "def get_default_range(book, split_equally = False):\n",
    "    d = {}\n",
    "    d[\"start_char\"] = 0\n",
    "    d['end_char'] = len(book)\n",
    "    d['chapters'] = [{\"start_char\": 0, \"end_char\": len(book)}]\n",
    "    \n",
    "    return [d]\n",
    "    \n",
    "\n",
    "def get_coref_clusters2(book):\n",
    "    '''\n",
    "        This function detects coref_clusters by splitting \n",
    "        the story to parts and clusters\n",
    "    '''\n",
    "    all_clusters = get_cluster_range(book)\n",
    "    \n",
    "    if not len(all_clusters):\n",
    "        print(\"no chapters detect.\")\n",
    "        all_clusters = get_default_range(book, split_equally = False)\n",
    "\n",
    "    for i, part in enumerate(all_clusters):\n",
    "        for j, chapter in enumerate(part['chapters']):\n",
    "            print(i,j)\n",
    "            coref_res = coref_predictor.predict(book[chapter['start_char']:chapter['end_char']])\n",
    "            #chapter['clusters'] =  coref_res['clusters']\n",
    "            s = chapter['start_char']\n",
    "            e = chapter['end_char']\n",
    "            local_doc = nlp(book[s:e])\n",
    "            token_offset = []\n",
    "            for token in local_doc:\n",
    "                token_offset.append(token.idx)\n",
    "                \n",
    "            adjusted_cluster = []\n",
    "            for cl in coref_res['clusters']:\n",
    "                new_cl = []\n",
    "                for c in cl:\n",
    "                    cs = token_offset[c[0]]+ s\n",
    "                    ce = token_offset[c[1] + 1]+ s\n",
    "                    new_cl.append([cs, ce, c[0], c[1] + 1])\n",
    "                adjusted_cluster.append(new_cl)\n",
    "                \n",
    "            chapter['clusters'] = adjusted_cluster\n",
    "\n",
    "    return all_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no chapters detect.\n",
      "0 0\n"
     ]
    }
   ],
   "source": [
    "all_clusters = get_coref_clusters2(book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_clusters[0]['chapters'][36]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('../static/data/story/'+story+ '/' + story +'_annotation.json', 'w') as outfile:\n",
    "    outfile.write(json.dumps(all_clusters, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load annotation and characters from json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "f = open( '../static/data/story/'+story+ '/' + story + '_annotation.json')\n",
    "all_clusters = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open( '../static/data/story/'+story+ '/' + story + '_characters.json')\n",
    "data = json.load(f)\n",
    "\n",
    "characters = {}\n",
    "tags = {}\n",
    "for k in data:\n",
    "    if data[k]['type'] == 'character':\n",
    "        characters[data[k]['name']] = data[k]['data']\n",
    "    elif data[k]['type'] == 'tag':\n",
    "        tags[data[k]['name']] = data[k]['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "# characters.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment and Emotion analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "from transformers import AutoModelForSequenceClassification, TFAutoModelForSequenceClassification\n",
    "from scipy.special import softmax\n",
    "import csv\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(f\"cardiffnlp/twitter-roberta-base-emotion\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(f\"cardiffnlp/twitter-roberta-base-emotion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download label mapping\n",
    "mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/emotion/mapping.txt\"\n",
    "with urllib.request.urlopen(mapping_link) as f:\n",
    "    html = f.read().decode('utf-8').split(\"\\n\")\n",
    "    csvreader = csv.reader(html, delimiter='\\t')\n",
    "labels = [row[1] for row in csvreader if len(row) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emotion(text):\n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "    output = model(**encoded_input)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    ranking = np.argsort(scores)\n",
    "    ranking = ranking[::-1]\n",
    "\n",
    "    l = labels[ranking[0]]\n",
    "        \n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n"
     ]
    }
   ],
   "source": [
    "sentence_mapping = []\n",
    "global_ind = 0\n",
    "for i, part in enumerate(all_clusters):\n",
    "    for j, chapter in enumerate(part['chapters']):\n",
    "        print(i,j)\n",
    "#         if j < 17:\n",
    "        s = chapter['start_char']\n",
    "        e = chapter['end_char']\n",
    "        local_doc = nlp(book[s:e])\n",
    "        local_ind = 0\n",
    "        for sent in local_doc.sents:\n",
    "            d = {}\n",
    "            d['text'] = sent.text\n",
    "            d['clean_text'] = \" \".join(sent.text.split())\n",
    "            d['sentiment'] = sentiment_predictor.predict(d['clean_text'])['probs'][0]\n",
    "            d['emotion'] = get_emotion(d['clean_text'])\n",
    "            d['start_char'] = s + sent.start_char\n",
    "            d['end_char'] = s + sent.end_char\n",
    "            d['part'] = i\n",
    "            d['chapter'] = j\n",
    "            d['global_sent_id'] = global_ind\n",
    "            d['local_sent_id'] = local_ind ## index within a chapter\n",
    "\n",
    "            d['mentions'] = []\n",
    "            for c in characters:\n",
    "                for mention in characters[c]:\n",
    "                    if (mention[0] >= (sent.start_char + s)) and (mention[1] < (sent.end_char + s)):\n",
    "                        d['mentions'].append({'start_char': mention[0], \n",
    "                                              'end_char': mention[1],\n",
    "                                              'character': c})\n",
    "\n",
    "            sentence_mapping.append(d)\n",
    "            global_ind += 1\n",
    "            local_ind += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '\\n\"The person you are calling has blocked your number.\"',\n",
       " 'clean_text': '\"The person you are calling has blocked your number.\"',\n",
       " 'sentiment': 0.0003652545274235308,\n",
       " 'emotion': 'anger',\n",
       " 'start_char': 22955,\n",
       " 'end_char': 23009,\n",
       " 'part': 0,\n",
       " 'chapter': 0,\n",
       " 'global_sent_id': 319,\n",
       " 'local_sent_id': 319,\n",
       " 'mentions': [{'start_char': 22996, 'end_char': 23001, 'character': 'Sophie'}]}"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_mapping[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open( '../static/data/story/'+story+ '/' + story + '_annotation.json', 'w') as outfile:\n",
    "    outfile.write(json.dumps(all_clusters, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open( '../static/data/story/'+story+ '/' + story + '_annotation.json')\n",
    "all_clusters = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_emotion_data = []\n",
    "\n",
    "for sent_id, sent in enumerate(sentence_mapping):\n",
    "    character_dict = {}\n",
    "    for mention in sent['mentions']:\n",
    "        ## only need one mention in a sentence, not all of them.\n",
    "        if mention['character'] not in character_dict:\n",
    "            d = {\"part\": sent['part'], \"chapter\": sent['chapter'],\n",
    "                 \"character\": mention['character'],\n",
    "                 \"start_char\": sent['start_char'],\n",
    "                 \"end_char\": sent['end_char'],\n",
    "                 \"global_sent_id\": sent['global_sent_id'],\n",
    "                 \"local_sent_id\": sent['local_sent_id'],\n",
    "                 \"sentiment\": sent['sentiment'],\n",
    "                 \"emotion\": sent['emotion'],\n",
    "                 \"presence\": 1}\n",
    "\n",
    "            sentiment_emotion_data.append(d)\n",
    "            character_dict[mention['character']] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"sentiment_emotion_data\": sentiment_emotion_data, \n",
    "        \"characters\": list(characters.keys()),\n",
    "        \"characters_map\": characters,\n",
    "        \"sentence_mapping\": sentence_mapping}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['chapters_map'] = all_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### presence of tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_presence = []\n",
    "for sent_id, sent in enumerate(sentence_mapping):   \n",
    "    for t in tags:\n",
    "        flag = False\n",
    "        for mention in tags[t]:\n",
    "            if (mention[0] >= sent['start_char']) and (mention[1] <= sent['end_char']):\n",
    "                d = {\"part\": sent['part'], \"chapter\": sent['chapter'],\n",
    "                     \"tag\": t,\n",
    "                     \"start_char\": sent['start_char'],\n",
    "                     \"end_char\": sent['end_char'],\n",
    "                     \"global_sent_id\": sent['global_sent_id'], \n",
    "                     \"local_sent_id\": sent['local_sent_id'],\n",
    "                     \"presence\": 1}\n",
    "                tags_presence.append(d)\n",
    "                break;\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"tags_map\"] = tags\n",
    "data[\"tags_presence\"] = tags_presence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../static/data/story/'+story+ '/' + story +'.json', 'w') as outfile:\n",
    "    outfile.write(json.dumps(data, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### identify verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = nltk.stem.SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from scipy.spatial.distance import cosine \n",
    "trf_nlp = spacy.load(\"en_core_web_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "f = open( '../static/data/story/'+story+ '/' + story + '.json')\n",
    "data = json.load(f)\n",
    "sentence_mapping = data['sentence_mapping']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '\\n\\n\\n\\n\\n',\n",
       " 'clean_text': '',\n",
       " 'sentiment': 0.47096481919288635,\n",
       " 'emotion': 'anger',\n",
       " 'start_char': 259993,\n",
       " 'end_char': 259998,\n",
       " 'part': 0,\n",
       " 'chapter': 16,\n",
       " 'global_sent_id': 3138,\n",
       " 'local_sent_id': 175,\n",
       " 'mentions': []}"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_mapping[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_frame(arg_type, arg_text, s, e, c=False):\n",
    "    d = {\n",
    "        'arg_type': arg_type,\n",
    "        'arg_text': arg_text , \n",
    "        'start_char': s,\n",
    "        'start_end': e\n",
    "    }\n",
    "    if c:\n",
    "        d['character'] = c\n",
    "        \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linalg as LA\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine \n",
    "def get_bert_embedding(text, v):\n",
    "    try:\n",
    "        temp_doc = trf_nlp(text)\n",
    "        v_ind = -1\n",
    "        for i, t in enumerate(temp_doc):\n",
    "            if t.text == v:\n",
    "                v_ind = i\n",
    "    #             print(\"#### matched\")\n",
    "    #             print(text, v)\n",
    "\n",
    "        if v_ind >= 0:\n",
    "            #print(v_ind)\n",
    "            bert_tokens = temp_doc._.trf_data.align[v_ind].data\n",
    "            #print(bert_tokens)\n",
    "            embds = temp_doc._.trf_data.tensors[0]\n",
    "            embds = embds.reshape(-1, embds.shape[-1])\n",
    "            a = np.zeros(768)\n",
    "            for i, t in enumerate(bert_tokens):\n",
    "                a = np.add(a, embds[t])\n",
    "\n",
    "            return a/LA.norm(a)\n",
    "        else:\n",
    "            print(\"#### did not matched\")\n",
    "            print(text, v)\n",
    "\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(\"Exception\")\n",
    "        print(text, v)\n",
    "        \n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function for getting embedding for a verb using GLOVE\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import gensim.downloader as api\n",
    "\n",
    "wv = api.load('glove-wiki-gigaword-50')\n",
    "\n",
    "def get_glove_embedding(text, v):\n",
    "    try:\n",
    "        return wv[v]\n",
    "    except Exception as e:\n",
    "        print(\"Exception\")\n",
    "        print(text, v)\n",
    "        \n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_verb_embedding(\"Build something\", \"Build\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception\n",
      " Glancing\n",
      "100\n",
      "Exception\n",
      " Enjoying\n",
      "200\n",
      "Exception\n",
      " Dumping\n"
     ]
    }
   ],
   "source": [
    "frames = []\n",
    "frame = \"\"\n",
    "\n",
    "for sent_id, sent in enumerate(sentence_mapping): \n",
    "    sent_doc = nlp(sent['text'])\n",
    "    \n",
    "    token_offset = []\n",
    "    tokens = []     \n",
    "    c = 0\n",
    "    for tok in sent_doc:\n",
    "        if \"\\n\" in tok.text:\n",
    "            c += 1\n",
    "        if \"\\n\" not in tok.text:\n",
    "            #print(tok.text, c)\n",
    "            token_offset.append([tok.idx, c])\n",
    "            tokens.append(tok)\n",
    "\n",
    "    ## predict oie\n",
    "    oie = open_info_predictor.predict(sent['text'])\n",
    "    words = oie['words']\n",
    "\n",
    "    for res in oie['verbs']:\n",
    "        if ((res['verb'] in stops)):\n",
    "            continue\n",
    "\n",
    "        ## initialize object for current frame\n",
    "        frame = {\"part\": sent['part'], \"chapter\": sent['chapter'],\n",
    "                 \"global_sent_id\": sent['global_sent_id'], \"local_sent_id\": sent['local_sent_id'],\n",
    "                 'verb': res['verb'], 'lemmatized_verb':stemmer.stem(res['verb']), \n",
    "                 'args': [], 'got_character_as_agent': []}\n",
    "        temp = \"\"\n",
    "\n",
    "        for t_id, t in enumerate(res['tags']):\n",
    "            if t != 'O':\n",
    "                spl = t.split('-')\n",
    "                if ((spl[0] == 'B')): ## tag starts with a 'B' and has ARG0, ARG1, etc. start of a potential match\n",
    "                    if len(temp): ## there is something to save\n",
    "                        tok_start = token_offset[arg_s][0] +  sent['start_char']\n",
    "                        tok_end = token_offset[t_id][0] +  sent['start_char']\n",
    "                        for mention in sent['mentions']:\n",
    "                            if ( (mention['start_char'] >= tok_start) and \\\n",
    "                                     (mention['end_char'] <= tok_end) and \\\n",
    "                                     ('ARG0' in saved_type )):\n",
    "\n",
    "                                    frame['got_character_as_agent'].append(mention['character'])\n",
    "                                    break;\n",
    "\n",
    "                        if abs(t_id - arg_s) < 12:\n",
    "                            frame['args'].append(create_frame(saved_type, temp, tok_start, tok_end))\n",
    "\n",
    "                    ## save the word, argument type, and index\n",
    "                    temp = words[t_id]\n",
    "                    saved_type = spl[1:]\n",
    "                    arg_s = t_id\n",
    "                    #print(\"1\", temp, saved_type, arg_s)\n",
    "\n",
    "                ## if tag starts with an \"I\", we are inside \n",
    "                ## a continuous argument\n",
    "                elif ((spl[0] == 'I')):\n",
    "\n",
    "                    ## append to existing argument\n",
    "                    temp += ' '+words[t_id]\n",
    "\n",
    "            else:\n",
    "                ## check for any unsaved frame\n",
    "                if len(temp): ## there is something to save\n",
    "                    tok_start = token_offset[arg_s][0]  + sent['start_char']\n",
    "                    tok_end = token_offset[t_id][0]  + sent['start_char']\n",
    "                    for mention in sent['mentions']:\n",
    "                        if ( (mention['start_char'] >= tok_start) and \\\n",
    "                                 (mention['end_char'] <= tok_end) and \\\n",
    "                                 ('ARG0' in saved_type )):\n",
    "\n",
    "                                frame['got_character_as_agent'].append(mention['character'])\n",
    "                                break;\n",
    "\n",
    "                    if abs(t_id - arg_s) < 12:\n",
    "                        frame['args'].append(create_frame(saved_type, temp, tok_start, tok_end))    \n",
    "\n",
    "                ## reinitialize temp to empty string\n",
    "                temp = \"\"\n",
    "\n",
    "\n",
    "        if ( (len(frame['args'])) and (len(frame['got_character_as_agent'])) and (len(frame['verb']) >=4) ):\n",
    "            emd = get_glove_embedding(\"\", frame['verb'])\n",
    "            if len(emd):\n",
    "                frame['embedding'] = emd\n",
    "            \n",
    "                frames.append(frame)\n",
    "\n",
    "                if len(frames) % 100 == 0:\n",
    "                    print(len(frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in frames:\n",
    "    f['embedding'] = f['embedding'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for f in frames:\n",
    "#     f['embedding'] = get_glove_embedding(\"\", f['verb']).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['frames'] = frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frames[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''It was a very fine November day, and the Miss Musgroves came through\n",
    "the little grounds, and stopped for no other purpose than to say, that\n",
    "they were going to take a long walk, and therefore concluded Mary could\n",
    "not like to go with them; and when Mary immediately replied, with some\n",
    "jealousy at not being supposed a good walker, \"Oh, yes, I should like\n",
    "to join you very much, I am very fond of a long walk;\" Anne felt\n",
    "persuaded, by the looks of the two girls, that it was precisely what\n",
    "they did not wish, and admired again the sort of necessity which the\n",
    "family habits seemed to produce, of everything being to be\n",
    "communicated, and everything being to be done together, however\n",
    "undesired and inconvenient.'''\n",
    "v = \"persuaded\"\n",
    "emd = get_verb_embedding(text, \"persuaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frames = data['frames']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapters = data['chapters_map'][0]['chapters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_frames(part, chapter, c):\n",
    "    res = [frame for frame in frames if frame['part'] == part \\\n",
    "           and frame['chapter'] == chapter \\\n",
    "           and (c in frame['got_character_as_agent']) and (len(frame['embedding']))]\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count_matrix(curr_frames, prev_frames):\n",
    "    l1 = len(curr_frames)\n",
    "    l2 = len(prev_frames)\n",
    "    m = np.zeros((l1, l2))\n",
    "    for i1 in range(l1):\n",
    "        for j1 in range(l2):\n",
    "            try:\n",
    "                d = cosine(curr_frames[i1]['embedding'], prev_frames[j1]['embedding'])\n",
    "                if d > 1:\n",
    "                    d = 1\n",
    "                m[i1][j1] = d\n",
    "            except Exception as e:\n",
    "                print(i1,j1)\n",
    "                \n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_frames = filter_frames(0,  1, \"Sir Walter Elliot\")\n",
    "prev_frames = filter_frames(0,  0, \"Sir Walter Elliot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = get_count_matrix(curr_frames, prev_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41403920161581365"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m[8][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'remove'"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_frames[8]['verb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = []\n",
    "for i, part in enumerate(all_clusters):\n",
    "    for j, chapter in enumerate(part['chapters']):\n",
    "#         if j < 17:\n",
    "            for c in characters:\n",
    "                #print(i, j, c)\n",
    "                if j > 0:\n",
    "                    curr_frames = filter_frames(i,  j, c)\n",
    "                    prev_frames = filter_frames(i,  j-1, c)\n",
    "                    if (j == 0) or (len(curr_frames) == 0) or (len(prev_frames) == 0):\n",
    "                        d = {\"part\": i, \"chapter\":j, \"character\": c, \n",
    "                             'start_char': chapters[j]['start_char'], 'end_char': chapters[j]['end_char'],\n",
    "                             'change_in_action': 0}\n",
    "                        actions.append(d)\n",
    "                    else:\n",
    "                        m = get_count_matrix(curr_frames, prev_frames)\n",
    "                        # Convert it into a 1D array\n",
    "                        a_1d = m.flatten()\n",
    "\n",
    "                        # Find the indices in the 1D array\n",
    "                        idx_1d = a_1d.argsort()[::-1]\n",
    "\n",
    "                        # convert the idx_1d back into indices arrays for each dimension\n",
    "                        x_idx, y_idx = np.unravel_index(idx_1d, m.shape)\n",
    "\n",
    "                        _, idx = np.unique(x_idx, return_index=True)\n",
    "                        x_idx = x_idx[np.sort(idx)]\n",
    "\n",
    "                        for i1, index in enumerate(x_idx):\n",
    "                            curr_frames[index]['sorted_idx'] = i1\n",
    "    #                         curr_frames[index]['embedding'] = []\n",
    "\n",
    "                        _, idx = np.unique(y_idx, return_index=True)\n",
    "                        y_idx = y_idx[np.sort(idx)]\n",
    "\n",
    "                        for i1, index in enumerate(y_idx):\n",
    "                            prev_frames[index]['sorted_idx'] = i1\n",
    "    #                         prev_frames[index]['embedding'] = []\n",
    "    #                         prev_frames[index].pop('embedding')\n",
    "\n",
    "                        d = {\"part\": i, \"chapter\":j, \"character\": c, \n",
    "                             'start_char': chapters[j]['start_char'], 'end_char': chapters[j]['end_char'],\n",
    "                             'change_in_action': float(np.mean(m)),\n",
    "                             'curr_frames': curr_frames, 'prev_frames': prev_frames }\n",
    "                        actions.append(d)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['actions'] = actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = data['frames']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_only = []\n",
    "for f in frames:\n",
    "    sent = sentence_mapping[f['global_sent_id']]\n",
    "    for arg in f['args']:\n",
    "        if 'V' in arg['arg_type']:\n",
    "            actions_only.append({\n",
    "                \"actions_only\": arg['arg_text'],\n",
    "                \"sent_start_char\": sent['start_char'],\n",
    "                \"sent_end_char\": sent['end_char'],\n",
    "                \"character\": f['got_character_as_agent'][0],\n",
    "                \"action_idx\":(arg['start_char'], arg['start_end'])})\n",
    "    #adj[t.text] = tmp #h.text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_emd = [f['embedding'] for f in frames]\n",
    "all_words = [f['verb'] for f in frames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.array(all_emd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(\n",
    "    n_clusters=5, init='random',\n",
    "    n_init=10, max_iter=300, \n",
    "    tol=1e-04, random_state=0\n",
    ")\n",
    "y_km = km.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, y in enumerate(y_km):\n",
    "    actions_only[i]['category'] = int(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'actions_only': 'checked',\n",
       "  'sent_start_char': 234,\n",
       "  'sent_end_char': 268,\n",
       "  'character': 'Sophie',\n",
       "  'action_idx': (251, 259),\n",
       "  'category': 0},\n",
       " {'actions_only': 'busied',\n",
       "  'sent_start_char': 777,\n",
       "  'sent_end_char': 847,\n",
       "  'character': 'Sophie',\n",
       "  'action_idx': (781, 788),\n",
       "  'category': 4},\n",
       " {'actions_only': 'resolving',\n",
       "  'sent_start_char': 777,\n",
       "  'sent_end_char': 847,\n",
       "  'character': 'Sophie',\n",
       "  'action_idx': (807, 817),\n",
       "  'category': 2},\n",
       " {'actions_only': 'survive',\n",
       "  'sent_start_char': 777,\n",
       "  'sent_end_char': 847,\n",
       "  'character': 'Sophie',\n",
       "  'action_idx': (827, 835),\n",
       "  'category': 2},\n",
       " {'actions_only': 'liked',\n",
       "  'sent_start_char': 848,\n",
       "  'sent_end_char': 1157,\n",
       "  'character': 'Sophie',\n",
       "  'action_idx': (855, 861),\n",
       "  'category': 3},\n",
       " {'actions_only': 'found',\n",
       "  'sent_start_char': 848,\n",
       "  'sent_end_char': 1157,\n",
       "  'character': 'Sophie',\n",
       "  'action_idx': (938, 944),\n",
       "  'category': 0},\n",
       " {'actions_only': 'loved',\n",
       "  'sent_start_char': 848,\n",
       "  'sent_end_char': 1157,\n",
       "  'character': 'Sophie',\n",
       "  'action_idx': (1023, 1029),\n",
       "  'category': 3},\n",
       " {'actions_only': 'clenched',\n",
       "  'sent_start_char': 848,\n",
       "  'sent_end_char': 1157,\n",
       "  'character': 'Sophie',\n",
       "  'action_idx': (1068, 1077),\n",
       "  'category': 4},\n",
       " {'actions_only': 'assembling',\n",
       "  'sent_start_char': 848,\n",
       "  'sent_end_char': 1157,\n",
       "  'character': 'Sophie',\n",
       "  'action_idx': (1103, 1114),\n",
       "  'category': 0},\n",
       " {'actions_only': 'resume',\n",
       "  'sent_start_char': 1303,\n",
       "  'sent_end_char': 1413,\n",
       "  'character': 'Sophie',\n",
       "  'action_idx': (1324, 1331),\n",
       "  'category': 2}]"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions_only[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['actions_only'] = actions_only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex3 = '\"([^\"]*)\"'\n",
    "regex4 = '\"I\\s+|\\s+I\\s+|\\s+me\\s+|\\s+my\\s+|\\s+me(;|.|,|!|\\?)'\n",
    "\n",
    "ds = []\n",
    "\n",
    "for i, part in enumerate(all_clusters):\n",
    "    for j, chapter in enumerate(part['chapters']):\n",
    "#         if j < 17:\n",
    "            s = chapter['start_char']\n",
    "            e = chapter['end_char']\n",
    "            ds_dict = {}\n",
    "            for m in re.finditer(regex3, book[s:e]):\n",
    "                m_span = m.span()\n",
    "                #print(\"......\",m.group())\n",
    "                d = {'part':i, 'chapter':j,\n",
    "                       'start_char': m_span[0] + s,\n",
    "                       'end_char': m_span[1] + s,\n",
    "                       'discourse_text': m.group(),\n",
    "                       'direct_discourse': 1}\n",
    "\n",
    "                for sub_m in re.finditer(regex4, m.group()):\n",
    "                    sub_m_span = sub_m.span()\n",
    "                    for c in characters:\n",
    "                        for mention in characters[c]:\n",
    "                            if (mention[0] >= (s + m_span[0] + sub_m_span[0] - 2)) \\\n",
    "                                and (mention[1] < (s + m_span[0] + sub_m_span[1] + 2)) :\n",
    "\n",
    "                                if m_span not in ds_dict:\n",
    "                                    d['character'] =  c\n",
    "                                    d['mention_start'] = mention[0]\n",
    "                                    d['mention_end'] =  mention[1]\n",
    "                                    ds_dict[m_span] = c\n",
    "                                    break;\n",
    "#                 if 'character' in d:\n",
    "                ds.append(d)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'part': 0,\n",
       "  'chapter': 0,\n",
       "  'start_char': 2529,\n",
       "  'end_char': 2550,\n",
       "  'discourse_text': '\"It’s for my mother,\"',\n",
       "  'direct_discourse': 1},\n",
       " {'part': 0,\n",
       "  'chapter': 0,\n",
       "  'start_char': 2605,\n",
       "  'end_char': 2663,\n",
       "  'discourse_text': '\"I’m going\\nto make the cupcakes myself, all from scratch.\"',\n",
       "  'direct_discourse': 1},\n",
       " {'part': 0,\n",
       "  'chapter': 0,\n",
       "  'start_char': 2664,\n",
       "  'end_char': 2670,\n",
       "  'discourse_text': '\"Aww,\"',\n",
       "  'direct_discourse': 1},\n",
       " {'part': 0,\n",
       "  'chapter': 0,\n",
       "  'start_char': 2720,\n",
       "  'end_char': 2740,\n",
       "  'discourse_text': '\"What a Mama’s boy.\"',\n",
       "  'direct_discourse': 1},\n",
       " {'part': 0,\n",
       "  'chapter': 0,\n",
       "  'start_char': 2741,\n",
       "  'end_char': 2760,\n",
       "  'discourse_text': '\"Isn’t that sweet?\"',\n",
       "  'direct_discourse': 1},\n",
       " {'part': 0,\n",
       "  'chapter': 0,\n",
       "  'start_char': 2780,\n",
       "  'end_char': 2856,\n",
       "  'discourse_text': '\"I’ve never even seen you make that kind of effort\\nfor your fiancée before.\"',\n",
       "  'direct_discourse': 1},\n",
       " {'part': 0,\n",
       "  'chapter': 0,\n",
       "  'start_char': 4373,\n",
       "  'end_char': 4388,\n",
       "  'discourse_text': '\"Girls’ night!\"',\n",
       "  'direct_discourse': 1},\n",
       " {'part': 0,\n",
       "  'chapter': 0,\n",
       "  'start_char': 6850,\n",
       "  'end_char': 6901,\n",
       "  'discourse_text': '\"Hey, Beth’s mom! I didn’t know you were visiting,\"',\n",
       "  'direct_discourse': 1,\n",
       "  'character': 'Sophie',\n",
       "  'mention_start': 6868,\n",
       "  'mention_end': 6870},\n",
       " {'part': 0,\n",
       "  'chapter': 0,\n",
       "  'start_char': 7461,\n",
       "  'end_char': 7477,\n",
       "  'discourse_text': '\"Call me Paula!\"',\n",
       "  'direct_discourse': 1,\n",
       "  'character': 'Paula',\n",
       "  'mention_start': 7467,\n",
       "  'mention_end': 7470},\n",
       " {'part': 0,\n",
       "  'chapter': 0,\n",
       "  'start_char': 7494,\n",
       "  'end_char': 7586,\n",
       "  'discourse_text': '\"So you’re Sophie. You seem much more promising than\\nthe last friend Beth introduced me to.\"',\n",
       "  'direct_discourse': 1,\n",
       "  'character': 'Paula',\n",
       "  'mention_start': 7579,\n",
       "  'mention_end': 7582},\n",
       " {'part': 0,\n",
       "  'chapter': 0,\n",
       "  'start_char': 7654,\n",
       "  'end_char': 7784,\n",
       "  'discourse_text': '\"My mom’s staying\\nwith me for the time being! She’s been feeling lonely lately, and her arthritis has been bothering\\nher more -- \"',\n",
       "  'direct_discourse': 1,\n",
       "  'character': 'Beth',\n",
       "  'mention_start': 7677,\n",
       "  'mention_end': 7680},\n",
       " {'part': 0,\n",
       "  'chapter': 0,\n",
       "  'start_char': 7785,\n",
       "  'end_char': 7851,\n",
       "  'discourse_text': '\"Don’t worry, I plan on living for several more decades at least,\"',\n",
       "  'direct_discourse': 1,\n",
       "  'character': 'Paula',\n",
       "  'mention_start': 7799,\n",
       "  'mention_end': 7801},\n",
       " {'part': 0,\n",
       "  'chapter': 0,\n",
       "  'start_char': 7963,\n",
       "  'end_char': 8000,\n",
       "  'discourse_text': '\"It’s\\nwonderful to meet you, Sophie.\"',\n",
       "  'direct_discourse': 1},\n",
       " {'part': 0,\n",
       "  'chapter': 0,\n",
       "  'start_char': 8001,\n",
       "  'end_char': 8018,\n",
       "  'discourse_text': '\"You too, Paula!\"',\n",
       "  'direct_discourse': 1},\n",
       " {'part': 0,\n",
       "  'chapter': 0,\n",
       "  'start_char': 9334,\n",
       "  'end_char': 9447,\n",
       "  'discourse_text': '\"Beth, why didn’t you tell me how fashionable your friend was? My goodness, you are\\nmaking me feel underdressed.\"',\n",
       "  'direct_discourse': 1,\n",
       "  'character': 'Paula',\n",
       "  'mention_start': 9361,\n",
       "  'mention_end': 9364},\n",
       " {'part': 0,\n",
       "  'chapter': 0,\n",
       "  'start_char': 9529,\n",
       "  'end_char': 9601,\n",
       "  'discourse_text': '\"Come on, Mom, stop fishing for compliments. We all know you\\nlook good.\"',\n",
       "  'direct_discourse': 1},\n",
       " {'part': 0,\n",
       "  'chapter': 0,\n",
       "  'start_char': 9641,\n",
       "  'end_char': 9664,\n",
       "  'discourse_text': '\"You really do, Paula,\"',\n",
       "  'direct_discourse': 1},\n",
       " {'part': 0,\n",
       "  'chapter': 0,\n",
       "  'start_char': 10017,\n",
       "  'end_char': 10162,\n",
       "  'discourse_text': '\"And thank you -- I got these jeans from that thrift store around the corner for just eight\\ndollars. Actually, Beth introduced me to that store.\"',\n",
       "  'direct_discourse': 1,\n",
       "  'character': 'Sophie',\n",
       "  'mention_start': 10035,\n",
       "  'mention_end': 10037},\n",
       " {'part': 0,\n",
       "  'chapter': 0,\n",
       "  'start_char': 10163,\n",
       "  'end_char': 10209,\n",
       "  'discourse_text': '\"And I was the one who introduced Beth to it,\"',\n",
       "  'direct_discourse': 1,\n",
       "  'character': 'Paula',\n",
       "  'mention_start': 10168,\n",
       "  'mention_end': 10170},\n",
       " {'part': 0,\n",
       "  'chapter': 0,\n",
       "  'start_char': 10226,\n",
       "  'end_char': 10319,\n",
       "  'discourse_text': '\"In fact, I remember\\nbuying maternity clothes from that very store before she was even born.\"',\n",
       "  'direct_discourse': 1,\n",
       "  'character': 'Paula',\n",
       "  'mention_start': 10236,\n",
       "  'mention_end': 10238},\n",
       " {'part': 0,\n",
       "  'chapter': 0,\n",
       "  'start_char': 10489,\n",
       "  'end_char': 10534,\n",
       "  'discourse_text': '\"I knew the nineties would come back\\naround,\"',\n",
       "  'direct_discourse': 1,\n",
       "  'character': 'Paula',\n",
       "  'mention_start': 10490,\n",
       "  'mention_end': 10492},\n",
       " {'part': 0,\n",
       "  'chapter': 0,\n",
       "  'start_char': 10559,\n",
       "  'end_char': 10601,\n",
       "  'discourse_text': '\"That’s the way fashion, and life, works.\"',\n",
       "  'direct_discourse': 1},\n",
       " {'part': 0,\n",
       "  'chapter': 0,\n",
       "  'start_char': 10602,\n",
       "  'end_char': 10641,\n",
       "  'discourse_text': '\"I’ve actually always worn this style,\"',\n",
       "  'direct_discourse': 1},\n",
       " {'part': 0,\n",
       "  'chapter': 0,\n",
       "  'start_char': 10659,\n",
       "  'end_char': 10747,\n",
       "  'discourse_text': '\"Even back when it made me\\ntotally stand out from a crowd. I just like the way it fits.\"',\n",
       "  'direct_discourse': 1,\n",
       "  'character': 'Sophie',\n",
       "  'mention_start': 10683,\n",
       "  'mention_end': 10685},\n",
       " {'part': 0,\n",
       "  'chapter': 0,\n",
       "  'start_char': 10748,\n",
       "  'end_char': 10807,\n",
       "  'discourse_text': '\"And it fits you perfectly! Just right for your body type!\"',\n",
       "  'direct_discourse': 1},\n",
       " {'part': 0,\n",
       "  'chapter': 0,\n",
       "  'start_char': 11132,\n",
       "  'end_char': 11167,\n",
       "  'discourse_text': '\"When she says that, she means it,\"',\n",
       "  'direct_discourse': 1},\n",
       " {'part': 0,\n",
       "  'chapter': 0,\n",
       "  'start_char': 11262,\n",
       "  'end_char': 11301,\n",
       "  'discourse_text': '\"One\\nlarge peanut butter fudge sundae!\"',\n",
       "  'direct_discourse': 1},\n",
       " {'part': 0,\n",
       "  'chapter': 0,\n",
       "  'start_char': 11518,\n",
       "  'end_char': 11581,\n",
       "  'discourse_text': '\"Oh! And some utensils for our lovely friend Sophie over here,\"',\n",
       "  'direct_discourse': 1},\n",
       " {'part': 0,\n",
       "  'chapter': 0,\n",
       "  'start_char': 11676,\n",
       "  'end_char': 11709,\n",
       "  'discourse_text': '\"And a drink. The\\ndrink’s on me.\"',\n",
       "  'direct_discourse': 1,\n",
       "  'character': 'Paula',\n",
       "  'mention_start': 11705,\n",
       "  'mention_end': 11707},\n",
       " {'part': 0,\n",
       "  'chapter': 0,\n",
       "  'start_char': 11815,\n",
       "  'end_char': 11835,\n",
       "  'discourse_text': '\"You do the honors.\"',\n",
       "  'direct_discourse': 1},\n",
       " {'part': 0,\n",
       "  'chapter': 0,\n",
       "  'start_char': 11836,\n",
       "  'end_char': 11857,\n",
       "  'discourse_text': '\"It’s our tradition,\"',\n",
       "  'direct_discourse': 1},\n",
       " {'part': 0,\n",
       "  'chapter': 0,\n",
       "  'start_char': 11874,\n",
       "  'end_char': 11906,\n",
       "  'discourse_text': '\"Dessert is the best appetizer.\"',\n",
       "  'direct_discourse': 1},\n",
       " {'part': 0,\n",
       "  'chapter': 0,\n",
       "  'start_char': 11907,\n",
       "  'end_char': 11946,\n",
       "  'discourse_text': '\"Thank you, but I can’t. I’m allergic,\"',\n",
       "  'direct_discourse': 1,\n",
       "  'character': 'Sophie',\n",
       "  'mention_start': 11923,\n",
       "  'mention_end': 11925},\n",
       " {'part': 0,\n",
       "  'chapter': 0,\n",
       "  'start_char': 12043,\n",
       "  'end_char': 12099,\n",
       "  'discourse_text': '\"Oh, that’s right, I can’t believe I forgot. I’m sorry.\"',\n",
       "  'direct_discourse': 1,\n",
       "  'character': 'Paula',\n",
       "  'mention_start': 12062,\n",
       "  'mention_end': 12064},\n",
       " {'part': 0,\n",
       "  'chapter': 0,\n",
       "  'start_char': 12135,\n",
       "  'end_char': 12166,\n",
       "  'discourse_text': '\"No, no, don’t worry about it,\"',\n",
       "  'direct_discourse': 1},\n",
       " {'part': 0,\n",
       "  'chapter': 0,\n",
       "  'start_char': 12582,\n",
       "  'end_char': 12650,\n",
       "  'discourse_text': '\"Beth, something’s happened between you and Justin, now, hasn’t it?\"',\n",
       "  'direct_discourse': 1},\n",
       " {'part': 0,\n",
       "  'chapter': 0,\n",
       "  'start_char': 12782,\n",
       "  'end_char': 12878,\n",
       "  'discourse_text': '\"I knew it -- you wouldn’t stop talking about him last time.\\nWhat happened? Tell me everything.\"',\n",
       "  'direct_discourse': 1,\n",
       "  'character': 'Paula',\n",
       "  'mention_start': 12783,\n",
       "  'mention_end': 12785},\n",
       " {'part': 0,\n",
       "  'chapter': 0,\n",
       "  'start_char': 14260,\n",
       "  'end_char': 14332,\n",
       "  'discourse_text': '\"You’re lucky to have a friend like Sophie, Beth. Wise and fashionable.\"',\n",
       "  'direct_discourse': 1},\n",
       " {'part': 0,\n",
       "  'chapter': 0,\n",
       "  'start_char': 14665,\n",
       "  'end_char': 14704,\n",
       "  'discourse_text': '\"Oh, no, is this\\n\\nyour peanut allergy?\"',\n",
       "  'direct_discourse': 1},\n",
       " {'part': 0,\n",
       "  'chapter': 0,\n",
       "  'start_char': 14734,\n",
       "  'end_char': 15146,\n",
       "  'discourse_text': '\"I didn’t know that watery eyes were a\\nsymptom; I thought they mostly manifested as rashes. Beth never had allergies growing up, and I\\nmust say that I don’t know what to do. I hope it isn’t serious. I’ll call the waitress to clear the\\nused plates right away. Or do you need medical assistance? I can call for that too; I know that\\nsome peanut allergies can be quite dangerous. I’m so sorry for our carelessness!\"',\n",
       "  'direct_discourse': 1,\n",
       "  'character': 'Paula',\n",
       "  'mention_start': 14735,\n",
       "  'mention_end': 14737},\n",
       " {'part': 0,\n",
       "  'chapter': 0,\n",
       "  'start_char': 15219,\n",
       "  'end_char': 15329,\n",
       "  'discourse_text': '\"No, Mom, Sophie\\nis fine. Her allergy is kind of weird, but it’s pretty much harmless. Stop embarrassing her!\"',\n",
       "  'direct_discourse': 1},\n",
       " {'part': 0,\n",
       "  'chapter': 0,\n",
       "  'start_char': 15531,\n",
       "  'end_char': 15635,\n",
       "  'discourse_text': '\"Oh,\\nall right, if you’re sure. Now, what did you say you discovered about Justin’s cybercrime\\nhistory?\"',\n",
       "  'direct_discourse': 1},\n",
       " {'part': 0,\n",
       "  'chapter': 0,\n",
       "  'start_char': 15636,\n",
       "  'end_char': 15667,\n",
       "  'discourse_text': '\"I know this one all too well,\"',\n",
       "  'direct_discourse': 1,\n",
       "  'character': 'Sophie',\n",
       "  'mention_start': 15637,\n",
       "  'mention_end': 15639},\n",
       " {'part': 0,\n",
       "  'chapter': 0,\n",
       "  'start_char': 16225,\n",
       "  'end_char': 16381,\n",
       "  'discourse_text': '\"It’s been wonderful\\nserving you all tonight. You have beautiful daughters. I’m so glad that they remember who the\\nmost important person in their lives is.\"',\n",
       "  'direct_discourse': 1},\n",
       " {'part': 0,\n",
       "  'chapter': 0,\n",
       "  'start_char': 16415,\n",
       "  'end_char': 16557,\n",
       "  'discourse_text': '\"My little one is in\\n\\nkindergarten right now, and I just hope to have this kind of relationship with her someday. Happy\\nMother’s Day, ladies.\"',\n",
       "  'direct_discourse': 1},\n",
       " {'part': 0,\n",
       "  'chapter': 0,\n",
       "  'start_char': 16587,\n",
       "  'end_char': 16797,\n",
       "  'discourse_text': '\"Well, that just warms my heart. I’m tipping her well.\\nWhere to next? I believe the thrift store is still open. We could catch that new horror movie that\\neverybody said was terrible. Or, we could hit the club…\"',\n",
       "  'direct_discourse': 1,\n",
       "  'character': 'Paula',\n",
       "  'mention_start': 16610,\n",
       "  'mention_end': 16613},\n",
       " {'part': 0,\n",
       "  'chapter': 0,\n",
       "  'start_char': 16798,\n",
       "  'end_char': 16804,\n",
       "  'discourse_text': '\"Mom,\"',\n",
       "  'direct_discourse': 1},\n",
       " {'part': 0,\n",
       "  'chapter': 0,\n",
       "  'start_char': 16821,\n",
       "  'end_char': 16830,\n",
       "  'discourse_text': '\"Really?\"',\n",
       "  'direct_discourse': 1},\n",
       " {'part': 0,\n",
       "  'chapter': 0,\n",
       "  'start_char': 16831,\n",
       "  'end_char': 16873,\n",
       "  'discourse_text': '\"You know I know how to have a good time,\"',\n",
       "  'direct_discourse': 1,\n",
       "  'character': 'Paula',\n",
       "  'mention_start': 16841,\n",
       "  'mention_end': 16843},\n",
       " {'part': 0,\n",
       "  'chapter': 0,\n",
       "  'start_char': 16900,\n",
       "  'end_char': 17042,\n",
       "  'discourse_text': '\"Now, we\\nshouldn’t keep Sophie here for too long. I’m sure her own mother is waiting on her. We can’t\\nsteal her, as much as we might like to!\"',\n",
       "  'direct_discourse': 1},\n",
       " {'part': 0,\n",
       "  'chapter': 0,\n",
       "  'start_char': 18079,\n",
       "  'end_char': 18172,\n",
       "  'discourse_text': '\"You’re right. It’s been a long day and I should get going. It was great meeting you,\\nPaula,\"',\n",
       "  'direct_discourse': 1,\n",
       "  'character': 'Sophie',\n",
       "  'mention_start': 18119,\n",
       "  'mention_end': 18121},\n",
       " {'part': 0,\n",
       "  'chapter': 0,\n",
       "  'start_char': 18298,\n",
       "  'end_char': 18520,\n",
       "  'discourse_text': '\"Not as great as meeting you, Sophie! I\\nhope to catch you on the pages of a fashion magazine next time. Or do you young people still\\nread magazines? I heard it’s all about that new app now. Beth, what was it called again?\"',\n",
       "  'direct_discourse': 1,\n",
       "  'character': 'Paula',\n",
       "  'mention_start': 18336,\n",
       "  'mention_end': 18337},\n",
       " {'part': 0,\n",
       "  'chapter': 0,\n",
       "  'start_char': 20530,\n",
       "  'end_char': 21611,\n",
       "  'discourse_text': '\"Hi, Sophie. Well, it’s me. Your mama. Oh, Sophie, sweetie, I can’t believe it’s your\\nbirthday again. I remember when I was your age, I was already returning to work after having\\nyou, trying to make a living for the both of us. You were a fussy child. I’m sure you’ve become\\ncalmer with time, and that your skin has cleared up. Oh, I’m just kidding. I could only have\\ndreamed of the kind of freedom that you have now. Anyway, sweetie, I miss you. I’m getting older,\\nand the doctor told me I don’t have much time left. I know you’re a grown woman now, but take it\\nfrom someone who has the experience: life is too short to hold onto anger. I do feel that it’s just\\ncruel to keep a dying woman from her only child, her sweet Sophie. Oh, the life I’ve lived. To\\nhave all my love swept under the rug and my few mistakes hung up to display. Sophie, I’m sorry\\nfor everything. Maybe I was a bit tough on you sometimes. I’m only human. Please forgive me; it\\nwas the best I could do. And now, I’d just like to see my only daughter again, before it’s too late.\\nI love you more than anything.\"',\n",
       "  'direct_discourse': 1,\n",
       "  'character': \"Sophie's Mother\",\n",
       "  'mention_start': 20554,\n",
       "  'mention_end': 20556},\n",
       " {'part': 0,\n",
       "  'chapter': 0,\n",
       "  'start_char': 22956,\n",
       "  'end_char': 23009,\n",
       "  'discourse_text': '\"The person you are calling has blocked your number.\"',\n",
       "  'direct_discourse': 1}]"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['direct_discourse_data'] = ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjectives(sent):\n",
    "    doc = nlp(sent['text'])\n",
    "    adj = []\n",
    "    for t in doc:\n",
    "        if t.pos_ == \"ADJ\":\n",
    "            h = t.head\n",
    "            found = False\n",
    "            #print(\"t: \", t.text, t.pos_)\n",
    "            while not found:\n",
    "                #print(\"h: \", h.text, h.pos_)\n",
    "                if h.pos_==\"VERB\":\n",
    "                    for c in h.children:\n",
    "                        #print(\"child: \", c.text, c.dep_)\n",
    "                        if str(c.dep_)==\"nsubj\":\n",
    "                            h = c\n",
    "                            noun = c.text\n",
    "                            noun_st_idx = c.idx\n",
    "                            if len(list(c.children))>0:\n",
    "                                noun = \" \".join([d.text for d in c.children]) + \" \" + noun\n",
    "                                noun_st_idx = list(c.children)[0].idx\n",
    "#                             print(\"Found! \",t.text, h.text)\n",
    "                            adj.append({\"adjective\":t.text,\n",
    "                                        \"embedding\": get_glove_embedding(\"\", t.text),\n",
    "                                        \"adjective_idx\":(t.idx + sent['start_char'], t.idx+ sent['start_char'] +len(t)), \n",
    "                                        \"subject\":noun, \n",
    "                                        \"subject_idx\":(noun_st_idx + sent['start_char'], noun_st_idx+ sent['start_char'] +len(noun))})\n",
    "                            #adj[t.text] = tmp #h.text\n",
    "                            found = True\n",
    "                            break\n",
    "\n",
    "                if h.text == h.head.text:\n",
    "#                     print(\"BREAK: \", h.text, h.head.text)\n",
    "                    break\n",
    "                h = h.head\n",
    "\n",
    "#             if found==False:\n",
    "#                 print(t.text, \" - Not Found!\")\n",
    "\n",
    "    #print(adj)\n",
    "    return adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception\n",
      " English\n",
      "[0.6050599813461304, 0.18190999329090118, 0.16393999755382538, -0.17321999371051788, 0.39002999663352966, 0.14191000163555145, -0.18749000132083893, 0.22791999578475952, -0.13433000445365906, -0.03973900154232979, -0.2924799919128418, -0.11434999853372574, -0.612280011177063, -0.11084000021219254, -0.08680599927902222, 0.4319700002670288, 0.46268999576568604, -0.9127900004386902, 0.0032685999758541584, -0.8772199749946594, -0.4365600049495697, 0.41110000014305115, 0.30052998661994934, -0.47099998593330383, 0.4814099967479706, -1.6452000141143799, -0.00877629965543747, 1.0281000137329102, 0.6123899817466736, -0.057787999510765076, 3.1659998893737793, -0.06929799914360046, -0.6797299981117249, -0.7535099983215332, -0.038444001227617264, -0.42412999272346497, 0.178849995136261, 0.2813200056552887, -0.38947999477386475, -0.2701700031757355, 0.2206300050020218, -0.018678000196814537, 0.18915000557899475, 0.16609999537467957, -0.20282000303268433, 0.10659000277519226, -0.3076600134372711, 0.7014999985694885, 0.4222100079059601, -0.35023999214172363]\n"
     ]
    }
   ],
   "source": [
    "adjective_data = []\n",
    "\n",
    "for sent_id, sent in enumerate(sentence_mapping):\n",
    "    for adj in adjectives(sent):\n",
    "        for mention in sent['mentions']:\n",
    "#             print(mention, adj['subject_idx'], adj['subject'])\n",
    "            try:\n",
    "                if len(adj['embedding']):\n",
    "                    if (adj['subject_idx'][0] >= mention['start_char']) and (adj['subject_idx'][1] <= mention['end_char']):\n",
    "                        adj['character'] = mention['character']\n",
    "                        adj[\"part\"] = sent['part']\n",
    "                        adj[\"chapter\"] =  sent['chapter']\n",
    "                        adj[\"sent_start_char\"] = sent['start_char']\n",
    "                        adj[\"sent_end_char\"] = sent['end_char']\n",
    "                        adj[\"global_sent_id\"] =  sent['global_sent_id']\n",
    "                        adj[\"local_sent_id\"] = sent['local_sent_id']\n",
    "                        adj['embedding'] = adj['embedding'].tolist()\n",
    "                        adjective_data.append(adj)\n",
    "            except Exception as e:\n",
    "                print(adj['embedding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'adjective': 'single',\n",
       " 'embedding': [-0.22134000062942505,\n",
       "  0.4932299852371216,\n",
       "  0.5309500098228455,\n",
       "  0.03496599942445755,\n",
       "  0.8447200059890747,\n",
       "  1.2430000305175781,\n",
       "  -0.6565399765968323,\n",
       "  0.11010000109672546,\n",
       "  0.5758799910545349,\n",
       "  0.5105100274085999,\n",
       "  0.42149001359939575,\n",
       "  -0.11423999816179276,\n",
       "  -0.32943999767303467,\n",
       "  1.2482999563217163,\n",
       "  0.2196899950504303,\n",
       "  -0.7825899720191956,\n",
       "  0.011030999943614006,\n",
       "  -0.25457999110221863,\n",
       "  -0.2687399983406067,\n",
       "  -0.4099299907684326,\n",
       "  -0.2869499921798706,\n",
       "  -0.7466400265693665,\n",
       "  0.08694600313901901,\n",
       "  0.47804000973701477,\n",
       "  -0.13755999505519867,\n",
       "  -1.0214999914169312,\n",
       "  -0.5839899778366089,\n",
       "  0.0527539998292923,\n",
       "  0.6379299759864807,\n",
       "  -0.5322800278663635,\n",
       "  3.6308999061584473,\n",
       "  -0.6694999933242798,\n",
       "  0.10321000218391418,\n",
       "  0.6220999956130981,\n",
       "  0.3382500112056732,\n",
       "  -0.19047999382019043,\n",
       "  1.2381000518798828,\n",
       "  -0.18893000483512878,\n",
       "  -0.32234999537467957,\n",
       "  -0.365090012550354,\n",
       "  -0.041478998959064484,\n",
       "  -0.608269989490509,\n",
       "  -0.5998499989509583,\n",
       "  -0.2566100060939789,\n",
       "  -0.597760021686554,\n",
       "  -0.37117999792099,\n",
       "  -0.05966100096702576,\n",
       "  -0.6676899790763855,\n",
       "  0.09798900038003922,\n",
       "  0.42921000719070435],\n",
       " 'adjective_idx': (3873, 3879),\n",
       " 'subject': 'Beth',\n",
       " 'subject_idx': (3861, 3865),\n",
       " 'character': 'Beth',\n",
       " 'part': 0,\n",
       " 'chapter': 0,\n",
       " 'sent_start_char': 3798,\n",
       " 'sent_end_char': 3905,\n",
       " 'global_sent_id': 44,\n",
       " 'local_sent_id': 44}"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjective_data[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_emd = [f['embedding'] for f in adjective_data]\n",
    "all_words = [f['adjective'] for f in adjective_data]\n",
    "\n",
    "X = np.array(all_emd)\n",
    "y_km = km.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, y in enumerate(y_km):\n",
    "    adjective_data[i]['category'] = int(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['adjective_data'] = adjective_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['sentiment_emotion_data', 'characters', 'characters_map', 'sentence_mapping', 'chapters_map', 'tags_map', 'tags_presence', 'frames', 'actions', 'actions_only', 'direct_discourse_data', 'adjective_data'])"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sent = \"The King was young, handsome, and wealthy; the Queen had a nature as good and gentle as her face was beautiful; and they adored one another, having married for love—which among kings and queens is not always the rule.\"\n",
    "# adjectives(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## combine all features (agency, sentiment) together\n",
    "for i, entry in enumerate(data['analyzed_data']):\n",
    "    a = agency[i]\n",
    "    d = {}\n",
    "    if ((a['part'] == entry['part']) and (a['chapter'] == entry['chapter']) and (a['character'] == entry['character'])):\n",
    "        entry['agency'] = a['agency']\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "f = open( '../static/data/story/'+story+ '/' + story + '.json')\n",
    "data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sentiment_emotion_data'] = sentiment_emotion_data\n",
    "data['sentence_mapping'] = sentence_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['chapters_map'] = all_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('../static/data/story/'+story+ '/'+ story +'.json', 'w') as outfile:\n",
    "    outfile.write(json.dumps(data, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
